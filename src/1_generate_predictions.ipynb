{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data generation\n",
    "\n",
    "This notebook creates MMLJobDescriptions - configurations for running training and inference based on the MML framework. MMLJobDescriptions may be rendered as bash scripts that contain the commands to start each of the MML experiments to generate the predictions. Alternatively it allows to start jobs via a runner. It suits both cases of a remote (LSF cluster) and local job submission. To run the notebook and the actual experiments one needs to install the MML framework ('mml-core'). In addition, 'mml-data' provides the necessary code to create the data locally. The provided MML plugin for the data splitting tag needs to be installed ('mml-prevalences').\n",
    "\n",
    "All resulting bash scripts will be stored in the \"training_scripts\" folder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import mml.interactive\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# initialize interactive MML usage\n",
    "mml.interactive.init(Path(mml.__file__).parent / 'mml.env')\n",
    "# import default runner and planning utils\n",
    "from mml.interactive.loading import get_task_structs\n",
    "from mml.interactive.planning import MMLJobDescription, SubprocessJobRunner\n",
    "\n",
    "# if mml-lsf is installed also LSF runner is imported\n",
    "try:\n",
    "    from mml_lsf.requirements import LSFSubmissionRequirements\n",
    "    from mml_lsf.runner import LSFJobRunner\n",
    "\n",
    "    LSF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LSFSubmissionRequirements = None\n",
    "    LSFJobRunner = None\n",
    "    LSF_AVAILABLE = False\n",
    "# list of tasks used in this study\n",
    "from prev.data_loading import all_tasks\n",
    "\n",
    "# setting the necessary paths\n",
    "current_path = os.getcwd()\n",
    "DATA_PATH = Path(current_path).parent / 'data'\n",
    "RESULT_PATH = Path(current_path).parent / 'results'\n",
    "assert DATA_PATH.exists() and RESULT_PATH.exists()\n",
    "TRAINING_SCRIPTS_PATH = Path(current_path) / 'training_scripts'\n",
    "# here we will write out commands\n",
    "os.chdir(TRAINING_SCRIPTS_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:55:31.532399Z",
     "start_time": "2024-08-06T10:55:26.335804Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration of the experiment system\n",
    "if LSF_AVAILABLE:\n",
    "    reqs = LSFSubmissionRequirements(num_gpus=1, vram_per_gpu=11.0, queue='gpu', script_name='mml_0_13.sh')\n",
    "else:\n",
    "    reqs = mml.interactive.planning.DefaultRequirements()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:55:53.041479Z",
     "start_time": "2024-08-06T10:55:53.036607Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data preparation"
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare steps\n",
    "create_cmds = list()\n",
    "tag_cmds = list()\n",
    "pp_cmds = list()\n",
    "# step one: task creation, download datasets, set up task descriptions\n",
    "create_cmds.append(MMLJobDescription(prefix_req=reqs, mode='create',\n",
    "                                     config_options={'task_list': all_tasks}))\n",
    "# step two: preprocess the data\n",
    "for t in all_tasks:\n",
    "    pp_cmds.append(MMLJobDescription(prefix_req=reqs, mode='pp',\n",
    "                                     config_options={'task_list': [t], 'preprocessing': 'default'}))\n",
    "# step three: redistribute the splits according to the strategy defined in the mml_plugin\n",
    "for t in all_tasks:\n",
    "    tag_cmds.append(MMLJobDescription(prefix_req=reqs, mode='info',\n",
    "                                      config_options={'task_list': [t], 'tagging.all': '+miccai?1337',\n",
    "                                                      'preprocessing': 'default'}))\n",
    "# generate descriptions\n",
    "mml.interactive.write_out_commands(cmd_list=create_cmds, name='01_create_cmds')\n",
    "mml.interactive.write_out_commands(cmd_list=pp_cmds, name='02_pp_cmds')\n",
    "mml.interactive.write_out_commands(cmd_list=tag_cmds, name='03_tag_cmds')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:55:55.337152Z",
     "start_time": "2024-08-06T10:55:55.327421Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## standard experiment repeated"
  },
  {
   "cell_type": "code",
   "source": [
    "# these reproduce the original computations with the updated mml backend\n",
    "project = 'mic23_predictions_reproduce'\n",
    "predict_cmds = list()\n",
    "for ix in [10, 11, 12, 13, 14, 15]:\n",
    "    for t in all_tasks:\n",
    "        opts = {'sampling.balanced': True, 'sampling.batch_size': 300, 'callbacks': 'early',\n",
    "                ' loss.auto_activate_weighing': False,\n",
    "                '+callbacks.early.patience': 7, 'mode.nested': False, 'mode.cv': False,\n",
    "                'lr_scheduler': 'plateau', 'lr_scheduler.patience': 5, 'mode.store_parameters': True, 'seed': ix,\n",
    "                'mode.subroutines': '[train,predict]', 'proj': f'{project}_{ix}',\n",
    "                'task_list': [f'{t}+miccai?1337', f'{t}+miccai?1337+nested?0'],\n",
    "                'pivot.name': f'{t}+miccai?1337', 'mode.eval_on': [f'{t}+miccai?1337', f'{t}+miccai?1337+nested?0'],\n",
    "                'trainer.max_epochs': 40, 'augmentations': 'baseline256',\n",
    "                'reuse.clean_up.parameters': True,\n",
    "                'preprocessing': 'default', 'trainer.min_epochs': 5}\n",
    "        predict_cmds.append(MMLJobDescription(prefix_req=reqs, config_options=opts, mode='train'))\n",
    "mml.interactive.write_out_commands(cmd_list=predict_cmds, name='04_reproduce_cmds')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:55:58.264204Z",
     "start_time": "2024-08-06T10:55:58.254019Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interactive job submission\n",
    "\n",
    "The new introduced runner class allows to submit / run locally the generated jobs alternatively to iterate over the generated command .txt files. "
   ]
  },
  {
   "cell_type": "code",
   "source": "runner = LSFJobRunner() if LSF_AVAILABLE else SubprocessJobRunner()",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T11:15:25.750001Z",
     "start_time": "2024-07-30T11:15:19.749025Z"
    }
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for job in predict_cmds:\n",
    "    runner.run(job)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## re-distribution ablation experiments\n",
    "the tag would have to replace the tags given in the prediction commands"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:56:03.301301Z",
     "start_time": "2024-08-06T10:56:03.298056Z"
    }
   },
   "cell_type": "code",
   "source": "dist_seeds = [3, 31, 314, 3141, 31415]",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:57:17.935689Z",
     "start_time": "2024-08-06T10:57:17.931300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alt_dist_seed_cmds = list()\n",
    "for seed in dist_seeds:\n",
    "    alt_dist_seed_cmds.append(MMLJobDescription(prefix_req=reqs, mode='info',\n",
    "                                                config_options={'task_list': all_tasks,\n",
    "                                                                'tagging.all': f'+miccai?{seed}',\n",
    "                                                                'preprocessing': 'default',\n",
    "                                                                'proj': 'default'}))\n",
    "mml.interactive.write_out_commands(cmd_list=alt_dist_seed_cmds, name='05_dataseed_cmds')"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:57:19.296769Z",
     "start_time": "2024-08-06T10:57:19.287716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project = 'mic23_predictions_datasplit_seed'\n",
    "predict_dataseed_cmds = list()\n",
    "for seed in dist_seeds:\n",
    "    for t in all_tasks:\n",
    "        opts = {'sampling.balanced': True, 'sampling.batch_size': 300, 'callbacks': 'early',\n",
    "                ' loss.auto_activate_weighing': False,\n",
    "                '+callbacks.early.patience': 7, 'mode.nested': False, 'mode.cv': False,\n",
    "                'lr_scheduler': 'plateau', 'lr_scheduler.patience': 5, 'mode.store_parameters': True, 'seed': 42,\n",
    "                # we keep this constant in these experiments\n",
    "                'mode.subroutines': '[train,predict]', 'proj': f'{project}_{seed}',\n",
    "                'task_list': [f'{t}+miccai?{seed}', f'{t}+miccai?{seed}+nested?0'],\n",
    "                'pivot.name': f'{t}+miccai?{seed}',\n",
    "                'mode.eval_on': [f'{t}+miccai?{seed}', f'{t}+miccai?{seed}+nested?0'],\n",
    "                'trainer.max_epochs': 40, 'augmentations': 'baseline256',\n",
    "                'reuse.clean_up.parameters': True,\n",
    "                'preprocessing': 'default', 'trainer.min_epochs': 5}\n",
    "        predict_dataseed_cmds.append(MMLJobDescription(prefix_req=reqs, config_options=opts, mode='train'))\n",
    "mml.interactive.write_out_commands(cmd_list=predict_dataseed_cmds, name='06_dataseed_predict_cmds')"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adapt loss weights during training \n",
    "\n",
    "In our extension we also tested the impact of adapting the cross entropy weights during training (both with perfectly known and  imperfectly estimated prevalences). For prevalence estimation we rely on results of the ACC method. These are stored as `24_prev_estimation.pkl` in the `results` folder."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:57:44.663608Z",
     "start_time": "2024-08-06T10:57:44.656072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def imbalance_ratio(class_prevalences: torch.Tensor) -> float:\n",
    "    \"\"\"Calculates the imbalance ratio.\"\"\"\n",
    "    min_frac = torch.min(class_prevalences)\n",
    "    max_frac = torch.max(class_prevalences)\n",
    "    return max_frac / min_frac\n",
    "\n",
    "\n",
    "def scale_prevalences_ir(prev_class_prevalences: torch.Tensor, ir: float = 1.):\n",
    "    \"\"\"Re-implements the same method in `prev.scaling` but returns the prevalences instead of data.\"\"\"\n",
    "    class_prevalences = copy.deepcopy(prev_class_prevalences)\n",
    "    # compute the original imbalance ratio\n",
    "    orig_ir = imbalance_ratio(class_prevalences)\n",
    "    min_frac = torch.min(class_prevalences)\n",
    "    max_frac = torch.max(class_prevalences)\n",
    "    # find index of class with maximal number of indices\n",
    "    max_class = torch.argmax(class_prevalences)\n",
    "    # iterate over classes\n",
    "    for i, value in enumerate(class_prevalences):\n",
    "        if ir >= orig_ir:\n",
    "            # downsample all but the max_class\n",
    "            if i != max_class:\n",
    "                class_prevalences[i] = (class_prevalences[i] * max_frac) / (\n",
    "                        min_frac * ir)  # undersample smaller classes\n",
    "        else:\n",
    "            # calculate the temperature\n",
    "            temp = (ir - 1) / (orig_ir - 1)\n",
    "            class_prevalences[i] = min_frac + temp * (class_prevalences[i] - min_frac)\n",
    "    class_prevalences = class_prevalences / class_prevalences.sum()\n",
    "    new_ir = imbalance_ratio(class_prevalences)\n",
    "    assert torch.isclose(new_ir, torch.tensor(ir)), f\"{ir=} {new_ir.item()=}\"\n",
    "    return class_prevalences"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:57:50.474473Z",
     "start_time": "2024-08-06T10:57:45.347303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gather prevalences in training data\n",
    "train_prevs = {}\n",
    "structs = get_task_structs(tasks=[t + '+miccai?1337' for t in all_tasks])\n",
    "with mml.interactive.default_file_manager():\n",
    "    for struct in structs:\n",
    "        name = struct.name.split('+')[0]\n",
    "        cls_occ = {idx: struct.class_occ[cls_name] for idx, cls_name in struct.idx_to_class.items()}\n",
    "        assert set(cls_occ.keys()) == set(range(len(cls_occ)))  # no subclasses ?\n",
    "        train_prevs[name] = torch.tensor([cls_occ[idx] for idx in range(len(cls_occ))],\n",
    "                                         dtype=torch.float) / struct.num_samples\n",
    "        assert train_prevs[name].sum() == 1"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:57:53.892906Z",
     "start_time": "2024-08-06T10:57:53.867865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load estimated prevalences - generated by notebook 3 on the original predictions\n",
    "estimated_prevalences = pd.read_pickle(RESULT_PATH / '24_prev_estimation_df.pkl').set_index(['task', 'ir'])['ACC']"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:57:55.152734Z",
     "start_time": "2024-08-06T10:57:55.147407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to determine cross entropy loss weights, when re-training with anticipated prevalence shift\n",
    "def get_loss_weights(task: str, target_ir: float, balanced_sampling: bool = False, estimated: bool = False):\n",
    "    current = train_prevs[task]\n",
    "    if estimated:\n",
    "        eval = torch.tensor(estimated_prevalences.loc[task, target_ir])\n",
    "    else:\n",
    "        eval = scale_prevalences_ir(current, target_ir)\n",
    "    if balanced_sampling:\n",
    "        weights = eval  # model will see each class equally often\n",
    "    else:\n",
    "        weights = eval / current  # model will see prevalent classes more often\n",
    "    return (weights / weights.sum()).tolist()"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:57:59.808036Z",
     "start_time": "2024-08-06T10:57:59.682242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adapted_retraining_cmds = list()\n",
    "for ix in range(1):  # we did not repeat this with multiple random seeds\n",
    "    for t in all_tasks:\n",
    "        for ir in [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0]:\n",
    "            opts = {'sampling.balanced': True, 'sampling.batch_size': 300, 'callbacks': 'early',\n",
    "                    ' loss.auto_activate_weighing': False,\n",
    "                    '+callbacks.early.patience': 7, 'mode.nested': False, 'mode.cv': False,\n",
    "                    'lr_scheduler': 'plateau', 'lr_scheduler.patience': 5, 'mode.store_parameters': True, 'seed': ix,\n",
    "                    'mode.subroutines': '[train,predict]', 'proj': f'mic23_predictions_extension_balanced_{ix}_{ir}',\n",
    "                    'task_list': [f'{t}+miccai?1337', f'{t}+miccai?1337+nested?0'],\n",
    "                    'pivot.name': f'{t}+miccai?1337', 'mode.eval_on': [f'{t}+miccai?1337', f'{t}+miccai?1337+nested?0'],\n",
    "                    'trainer.max_epochs': 40, 'augmentations': 'baseline256',\n",
    "                    'reuse.clean_up.parameters': True,\n",
    "                    'loss.class_weights': get_loss_weights(task=t, target_ir=ir, balanced_sampling=True),\n",
    "                    'preprocessing': 'default', 'trainer.min_epochs': 5}\n",
    "            adapted_retraining_cmds.append(MMLJobDescription(prefix_req=reqs, mode='train', config_options=opts))\n",
    "mml.interactive.write_out_commands(cmd_list=adapted_retraining_cmds, name='07_retraining_cmds')"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:58:01.913822Z",
     "start_time": "2024-08-06T10:58:01.863819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adapted_retraining_estimated_cmds = list()\n",
    "for ix in range(1):  # we did not repeat this with multiple random seeds\n",
    "    for t in all_tasks:\n",
    "        for ir in [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0]:\n",
    "            opts = {'sampling.balanced': True, 'sampling.batch_size': 300, 'callbacks': 'early',\n",
    "                    ' loss.auto_activate_weighing': False,\n",
    "                    '+callbacks.early.patience': 7, 'mode.nested': False, 'mode.cv': False,\n",
    "                    'lr_scheduler': 'plateau', 'lr_scheduler.patience': 5, 'mode.store_parameters': True, 'seed': ix,\n",
    "                    'mode.subroutines': '[train,predict]',\n",
    "                    'proj': f'mic23_predictions_extension_balanced_estimated_{ix}_{ir}',\n",
    "                    'task_list': [f'{t}+miccai?1337', f'{t}+miccai?1337+nested?0'],\n",
    "                    'pivot.name': f'{t}+miccai?1337', 'mode.eval_on': [f'{t}+miccai?1337', f'{t}+miccai?1337+nested?0'],\n",
    "                    'trainer.max_epochs': 40, 'augmentations': 'baseline256',\n",
    "                    'reuse.clean_up.parameters': True,\n",
    "                    'loss.class_weights': get_loss_weights(task=t, target_ir=ir, balanced_sampling=True,\n",
    "                                                           estimated=True),\n",
    "                    'preprocessing': 'default', 'trainer.min_epochs': 5}\n",
    "            adapted_retraining_estimated_cmds.append(\n",
    "                MMLJobDescription(prefix_req=reqs, mode='train', config_options=opts))\n",
    "mml.interactive.write_out_commands(cmd_list=adapted_retraining_estimated_cmds, name='08_retraining_estimated_cmds')"
   ],
   "execution_count": 15,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
