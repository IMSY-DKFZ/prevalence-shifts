{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RQ1: How well can we estimate prevalences from unlabeled deployment data?\n",
    "This notebook generates the figures 5 and C.11. It assesses quantification capabilities with existing methods in our use cases."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:01:21.006349Z",
     "start_time": "2024-08-21T14:01:19.041725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from progiter import ProgIter\n",
    "\n",
    "import quapy as qp\n",
    "from quapy.method.aggregative import CC, ACC, PCC, PACC, EMQ, KDEyCS, KDEyHD, KDEyML, DMy\n",
    "from quapy.error import nkld\n",
    "\n",
    "from src.prev.data_loading import get_values, Kind, Split, all_tasks\n",
    "from src.prev.plotting import plot_aggregate_results, Confidence, box_plot, multiplot\n",
    "from src.prev.scaling import scale_prevalences_ir\n",
    "from src.prev.quantification import QuantificationMethod, absolute_error, compute_w_hat_and_mu_hat, IdentityClassifier\n",
    "\n",
    "current_path = os.getcwd()\n",
    "DATA_PATH = Path(current_path).parent / 'data'\n",
    "RESULT_PATH = Path(current_path).parent / 'results'\n",
    "assert DATA_PATH.exists() and RESULT_PATH.exists()\n",
    "torch.manual_seed(seed=0)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0191b16290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:01:33.764716Z",
     "start_time": "2024-08-21T14:01:24.230729Z"
    }
   },
   "source": [
    "data = {}\n",
    "for t in ProgIter(all_tasks, desc='Loading data'):\n",
    "    data[t] = get_values(t, DATA_PATH, proj='mic23_predictions_original_0')  # original paper predictions"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data 100.00% 30/30... rate=3.15 Hz, eta=0:00:00, total=0:00:09\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:47:42.174099Z",
     "start_time": "2024-08-21T14:01:35.372308Z"
    }
   },
   "source": [
    "# Research Question 1: How well can we estimate prevalences from unlabeled deployment data?\n",
    "quantification_ir_results = []\n",
    "IRS = list(np.arange(1, 10.5, 0.5))\n",
    "for ir in IRS:\n",
    "    for t in ProgIter(all_tasks):\n",
    "        # modify DEV_TEST according to IR\n",
    "        try:\n",
    "            app_test_logits, app_test_classes = scale_prevalences_ir(logits=data[t][Kind.LOGITS][Split.APP_TEST],\n",
    "                                                                     classes=data[t][Kind.LABELS][Split.APP_TEST],\n",
    "                                                                     ir=ir)\n",
    "        except:\n",
    "            print(f'{t=}, {ir=}')\n",
    "            raise\n",
    "        mod_data = {Kind.LOGITS: {Split.DEV_CAL: data[t][Kind.LOGITS][Split.DEV_CAL],\n",
    "                                  Split.DEV_TEST: data[t][Kind.LOGITS][Split.DEV_TEST],\n",
    "                                  Split.APP_TEST: app_test_logits},\n",
    "                    Kind.LABELS: {Split.DEV_CAL: data[t][Kind.LABELS][Split.DEV_CAL],\n",
    "                                  Split.DEV_TEST: data[t][Kind.LABELS][Split.DEV_TEST],\n",
    "                                  Split.APP_TEST: app_test_classes}}\n",
    "        # estimate prevalence using BBSE\n",
    "        _, bbse_prior = compute_w_hat_and_mu_hat(mod_data[Kind.LABELS][Split.DEV_TEST],\n",
    "                                                 torch.argmax(mod_data[Kind.LOGITS][Split.DEV_TEST], dim=1),\n",
    "                                                 torch.argmax(mod_data[Kind.LOGITS][Split.APP_TEST], dim=1))\n",
    "        prior = (torch.bincount(app_test_classes) / len(app_test_classes)).numpy()\n",
    "        d_size = len(app_test_classes)\n",
    "        _info = {'ir': ir, 'task': t}\n",
    "        _info.update({\"BBSE\": bbse_prior})\n",
    "        _info.update({\"prior\": prior})\n",
    "        _info.update({\"d_size\": d_size})\n",
    "        # convert data to qp format\n",
    "        dev_data = qp.data.LabelledCollection(torch.softmax(mod_data[Kind.LOGITS][Split.DEV_TEST], dim=1),\n",
    "                                              mod_data[Kind.LABELS][Split.DEV_TEST])\n",
    "        app_data = qp.data.LabelledCollection(torch.softmax(mod_data[Kind.LOGITS][Split.APP_TEST], dim=1),\n",
    "                                              mod_data[Kind.LABELS][Split.APP_TEST])\n",
    "        dset = qp.data.base.Dataset(training=dev_data, test=app_data)\n",
    "        # compute estimated prevalences with methods from qp\n",
    "        for method_name, method in {\"CC\": CC, \"ACC\": ACC, \"PCC\": PCC, \"PACC\": PACC, \"EMQ\": EMQ, \"HDy\": DMy,\n",
    "                                    'KDEyCS': KDEyCS, 'KDEyHD': KDEyHD, 'KDEyML': KDEyML}.items():\n",
    "            identity_class = IdentityClassifier(len(prior))\n",
    "            model = method(identity_class)\n",
    "            model.fit(dset.training)\n",
    "            estim_prevalence = model.quantify(dset.test.instances)\n",
    "            _info.update({method_name: estim_prevalence})\n",
    "        quantification_ir_results.append(_info)\n",
    "ir_df = pd.DataFrame(quantification_ir_results)\n",
    "ir_df.to_pickle(RESULT_PATH / '24_prev_estimation_df.pkl')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.00% 30/30... rate=0.24 Hz, eta=0:00:00, total=0:02:06\n",
      " 100.00% 30/30... rate=0.21 Hz, eta=0:00:00, total=0:02:22\n",
      " 100.00% 30/30... rate=0.20 Hz, eta=0:00:00, total=0:02:26\n",
      " 100.00% 30/30... rate=0.20 Hz, eta=0:00:00, total=0:02:30\n",
      " 100.00% 30/30... rate=0.19 Hz, eta=0:00:00, total=0:02:35\n",
      " 100.00% 30/30... rate=0.19 Hz, eta=0:00:00, total=0:02:34\n",
      " 100.00% 30/30... rate=0.20 Hz, eta=0:00:00, total=0:02:33\n",
      " 100.00% 30/30... rate=0.20 Hz, eta=0:00:00, total=0:02:31\n",
      " 100.00% 30/30... rate=0.20 Hz, eta=0:00:00, total=0:02:30\n",
      " 100.00% 30/30... rate=0.20 Hz, eta=0:00:00, total=0:02:28\n",
      " 100.00% 30/30... rate=0.21 Hz, eta=0:00:00, total=0:02:25\n",
      " 100.00% 30/30... rate=0.20 Hz, eta=0:00:00, total=0:02:28\n",
      " 100.00% 30/30... rate=0.21 Hz, eta=0:00:00, total=0:02:24\n",
      " 100.00% 30/30... rate=0.21 Hz, eta=0:00:00, total=0:02:24\n",
      " 100.00% 30/30... rate=0.21 Hz, eta=0:00:00, total=0:02:21\n",
      " 100.00% 30/30... rate=0.21 Hz, eta=0:00:00, total=0:02:21\n",
      " 100.00% 30/30... rate=0.21 Hz, eta=0:00:00, total=0:02:21\n",
      " 100.00% 30/30... rate=0.22 Hz, eta=0:00:00, total=0:02:19\n",
      " 100.00% 30/30... rate=0.21 Hz, eta=0:00:00, total=0:02:21\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Figure 5 and Figure C.11"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:59:05.376677Z",
     "start_time": "2024-08-21T14:59:04.581718Z"
    }
   },
   "source": [
    "display_methods = [QuantificationMethod.CC, QuantificationMethod.EMQ, QuantificationMethod.PACC,\n",
    "                   QuantificationMethod.KDEyCS,\n",
    "                   QuantificationMethod.BBSE, QuantificationMethod.ACC, QuantificationMethod.DMy,\n",
    "                   QuantificationMethod.KDEyML, QuantificationMethod.KDEyHD]\n",
    "metrics = {\"Absolute error\": absolute_error, \"Normalized KLD\": nkld}\n",
    "limits = {\"Absolute error\": [0, 0.55], \"Normalized KLD\": [0, 0.17]}\n",
    "for metric_name, metric in metrics.items():\n",
    "    ir_df = pd.read_pickle(RESULT_PATH / '24_prev_estimation_df.pkl')\n",
    "    for col in ir_df.columns:\n",
    "        if col not in [\"ir\", \"task\", \"d_size\", 'prior']:\n",
    "            if metric_name == \"Normalized KLD\":\n",
    "                ir_df[col] = ir_df.apply(lambda row: metric(row['prior'], row[col], eps=1 / row['d_size']), axis=1)\n",
    "            else:\n",
    "                ir_df[col] = ir_df.apply(lambda row: metric(row['prior'], row[col]), axis=1)\n",
    "    fin_df = ir_df.groupby('task').aggregate(\n",
    "        {col: list for col in ir_df.columns if col not in ['task', 'd_size', 'prior']})\n",
    "    fin_df = fin_df.reset_index()\n",
    "    # select values at imbalance ratio 10\n",
    "    ir_10_df = ir_df.loc[ir_df['ir'] == 10][['task', *[q.value for q in display_methods]]]\n",
    "    # create line plot\n",
    "    subplt = plot_aggregate_results(fin_df, line_ids=display_methods, file=None,\n",
    "                                    delta=False, ci=Confidence.STD, y_axis_title=f\"<b>{metric_name}</b>\", title=None,\n",
    "                                    bound=[0, 0.2], opacity=0.15)\n",
    "    # create box plot\n",
    "    box1 = box_plot(ir_10_df, line_ids=display_methods)\n",
    "    # create final figure\n",
    "    fig = multiplot(rows=1, cols=2, subplts=[subplt, box1], horizontal_spacing=0.04, legend_index=1,\n",
    "                    y_title=f\"<b>{metric_name}</b>\", sub_x_axis_titles={0: \"Imbalance ratio\"},\n",
    "                    sub_y_ranges={0: limits[metric_name]}, shared_yaxes=True, vertical_spacing=0.1,\n",
    "                    ir_axes=[2], ir_values=[10], little_guys=True, icon_size=0.14, icon_y_adjustment=0.06)\n",
    "    # fig.show()\n",
    "    name = f\"24_estimating_prevalence_{metric_name}\"\n",
    "    fig.write_image(RESULT_PATH / f\"{name}.png\")\n",
    "    # fig.write_image(RESULT_PATH / f\"{name}.svg\")\n",
    "    # fig.write_image(RESULT_PATH / f\"{name}.pdf\")\n",
    "    # fig.write_html(RESULT_PATH / f\"{name}.html\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
